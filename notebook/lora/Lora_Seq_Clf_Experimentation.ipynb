{"cells":[{"cell_type":"markdown","metadata":{"id":"WELS7jmruZgQ"},"source":["## Setup Environment"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","import random\n","import sys\n","import warnings\n","from dataclasses import dataclass, field\n","from typing import List, Literal, Optional, Union\n","\n","import datasets\n","import evaluate\n","import numpy as np\n","from datasets import Value, load_dataset\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    PhiForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version, send_example_telemetry\n","from transformers.utils.versions import require_version\n","\n","from peft import (\n","    TaskType,\n","    LoraConfig,\n","    get_peft_model,\n","    PeftModel,\n","    PeftConfig,\n",")\n","\n","from dotenv import load_dotenv, find_dotenv\n","\n","load_dotenv(find_dotenv())\n","\n","HF_TOKEN = os.getenv(\"HF_TOKEN\")\n","WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n","os.environ[\"WANDB_PROJECT\"] = \"TEST_SEQ_CLASSIFICATION_RUNS\""]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["raw_datasets = load_dataset(\"MAdAiLab/twitter_disaster\",\"default\")\n","# raw_datasets = load_dataset(\"MAdAiLab/amazon-attrprompt\",\"default\")\n","# raw_datasets = load_dataset(\"coastalcph/lex_glue\", \"scotus\")\n","# raw_datasets = load_dataset(\"ccdv/patent-classification\",\"abstract\")\n","\n","# raw_datasets = df.rename_column(\"label\", \"labels\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logger = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def get_label_list(raw_dataset, split=\"train\") -> List[str]:\n","    \"\"\"Get the list of labels from a multi-label dataset\"\"\"\n","\n","    if isinstance(raw_dataset[split][\"label\"][0], list):\n","        label_list = [label for sample in raw_dataset[split][\"label\"] for label in sample]\n","        label_list = list(set(label_list))\n","    else:\n","        label_list = raw_dataset[split].unique(\"label\")\n","    # we will treat the label list as a list of string instead of int, consistent with model.config.label2id\n","    label_list = [str(label) for label in label_list]\n","    return label_list"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["label_list = get_label_list(raw_datasets, split=\"train\")\n","for split in [\"validation\", \"test\"]:\n","    if split in raw_datasets:\n","        val_or_test_labels = get_label_list(raw_datasets, split=split)\n","        diff = set(val_or_test_labels).difference(set(label_list))\n","        if len(diff) > 0:\n","            # add the labels that appear in val/test but not in train, throw a warning\n","            logger.warning(\n","                f\"Labels {diff} in {split} set but not in training set, adding them to the label list\"\n","            )\n","            label_list += list(diff)\n","# if label is -1, we throw a warning and remove it from the label list\n","for label in label_list:\n","    if label == -1:\n","        logger.warning(\"Label -1 found in label list, removing it.\")\n","        label_list.remove(label)\n","\n","label_list.sort()\n","num_labels = len(label_list)\n","if num_labels <= 1:\n","    raise ValueError(\"You need more than one label to do classification.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Load pretrained model and tokenizer"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a37ea858cc8469bafd443f79c3f93d0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["trainable params: 860,160 || all params: 7,111,528,448 || trainable%: 0.012095290151611583\n"]}],"source":["checkpoint = \"mistralai/Mistral-7B-v0.1\"\n","\n","config = AutoConfig.from_pretrained(\n","        pretrained_model_name_or_path=checkpoint,\n","        num_labels=num_labels,\n","        finetuning_task=\"text-classification\",\n","        trust_remote_code=True,\n",")\n","config.problem_type = \"single_label_classification\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    pretrained_model_name_or_path=checkpoint,\n","    trust_remote_code=True,\n",")\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","        pretrained_model_name_or_path=checkpoint,\n","        config=config,\n","        trust_remote_code=True,\n",")\n","model.resize_token_embeddings(len(tokenizer))\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","peft_config = LoraConfig(\n","            r=2,\n","            target_modules=[\"q_proj\", \"v_proj\"],\n","            lora_alpha=4,\n","            task_type=TaskType.SEQ_CLS,\n",")\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","label_to_id = {v: i for i, v in enumerate(label_list)}\n","\n","model.config.label2id = label_to_id\n","model.config.id2label = {id: label for label, id in label_to_id.items()}\n","\n","max_seq_length =  512 "]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b513418ec5404723a6603a2e2453f2e8","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on dataset:   0%|          | 0/1088 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Sample 6813 of the training set: {'text': 'Versions of KS where if a character was /every/ character world would explode.\\n\\nRin\\nShizune\\nMisha\\nEmi\\nKenji\\nYuuko\\nNomiya\\nHisao', 'label': 0, 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 12433, 594, 302, 524, 28735, 970, 513, 264, 3233, 403, 732, 19662, 28748, 3233, 1526, 682, 27930, 28723, 13, 13, 28754, 262, 13, 1981, 463, 1802, 13, 28755, 22674, 13, 28749, 3589, 13, 28796, 269, 3632, 13, 28802, 28718, 2950, 28709, 13, 28759, 16344, 5157, 13, 21860, 5044], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","Sample 7869 of the training set: {'text': 'RT AbbsWinston: #Zionist #Terrorist Demolish Tire Repair Shop Structure in #Bethlehem\\nhttp://t.co/ph2xLI8nVe http://t.co/22fuxHn7El', 'label': 0, 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 17059, 15859, 28713, 28780, 4138, 266, 28747, 422, 28828, 296, 392, 422, 28738, 1958, 392, 4267, 328, 789, 320, 536, 3357, 992, 14142, 3838, 8187, 297, 422, 28760, 761, 291, 6981, 13, 2872, 1508, 28707, 28723, 1115, 28748, 721, 28750, 28744, 4862, 28783, 28711, 28790, 28706, 3550, 1508, 28707, 28723, 1115, 28748, 28750, 28750, 28722, 1554, 28769, 28711, 28787, 5773], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n","Sample 7664 of the training set: {'text': \"'The Terrorist Tried to Get Out of the Car; I Shot Him' http://t.co/VSoxKbt6Nq\", 'label': 1, 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 464, 1014, 320, 1958, 392, 320, 1638, 298, 2483, 4655, 302, 272, 2364, 28745, 315, 1295, 322, 14649, 28742, 3550, 1508, 28707, 28723, 1115, 28748, 23999, 1142, 28796, 2695, 28784, 28759, 28775], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n"]}],"source":["def preprocess_function(examples):\n","    # Tokenize the texts\n","    result = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=max_seq_length, truncation=True)\n","\n","    return result\n","\n","raw_datasets = raw_datasets.map(\n","            preprocess_function,\n","            batched=True,\n","            desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = raw_datasets[\"train\"]\n","eval_dataset = raw_datasets[\"validation\"]\n","predict_dataset = raw_datasets[\"test\"]\n","\n","for index in random.sample(range(len(train_dataset)), 3):\n","    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n","\n","data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)"]},{"cell_type":"markdown","metadata":{},"source":["## Compute metric"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def compute_metrics(p: EvalPrediction):\n","    accuracy = evaluate.load(\"accuracy\")\n","    f1 = evaluate.load(\"f1\")\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    result = {\n","        \"accuracy\": accuracy.compute(predictions=preds, references=p.label_ids)[\"accuracy\"],\n","        \"f1_macro\": f1.compute(predictions=preds, references=p.label_ids, average=\"macro\")[\"f1\"],\n","        \"f1_micro\": f1.compute(predictions=preds, references=p.label_ids, average=\"micro\")[\"f1\"],\n","    }\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["## Training Args"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"5ziQ2bhFw4eD"},"outputs":[],"source":["training_args = TrainingArguments(\n","    do_train=False,\n","    do_eval=False,\n","    do_predict=True,\n","    bf16=True,\n","    fp16=False,\n","    gradient_checkpointing=True,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=50,\n","    save_steps=50,\n","    load_best_model_at_end=True,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=32,\n","    eval_accumulation_steps=100,\n","    max_grad_norm=1,\n","    weight_decay=0.1,\n","    optim=\"adamw_torch\",\n","    learning_rate=5e-6,\n","    lr_scheduler_type=\"linear\",\n","    num_train_epochs=3,\n","    report_to=\"wandb\",\n","    logging_strategy=\"steps\",\n","    logging_steps=10,\n","    save_total_limit=2,\n","    save_safetensors=False,\n","    overwrite_output_dir=True,\n","    log_level=\"warning\",\n","    output_dir=\"./test_runs/mistralai/Mistral-7B-v0.1/twitter_disaster\",\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/harpreet_guest2/akshat/Sequence_Classification/venv/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training, Evaluation and Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main(training_args):\n","    # Training\n","    if training_args.do_train:\n","        logger.info(\"*** Train ***\")\n","        train_result = trainer.train()\n","        metrics = train_result.metrics\n","        max_train_samples = len(train_dataset)\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n","        max_eval_samples = len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    # Predict\n","    if training_args.do_predict:\n","        logger.info(\"*** Predict ***\")\n","        predictions = trainer.predict(predict_dataset)\n","        metrics[\"test_samples\"] = len(predict_dataset)\n","        trainer.log_metrics(\"test\", predictions.metrics)\n","        trainer.save_metrics(\"test\", predictions.metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["main(training_args)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["del model\n","# del trainer"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","import torch\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Lora Model Inference"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["2"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["label_list = get_label_list(raw_datasets, split=\"train\")\n","num_labels = len(label_list)\n","num_labels"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fbee12a80454876b2d7a2e0b757b0a3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load the model\n","label_list = get_label_list(raw_datasets, split=\"train\")\n","num_labels = len(label_list)\n","\n","\n","peft_model_id = \"../../experiments_checkpoints/twitter_disaster\"\n","adapter_config = PeftConfig.from_pretrained(peft_model_id)\n","config = AutoConfig.from_pretrained(\n","        adapter_config.base_model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task=\"text-classification\",\n","        trust_remote_code=True,\n","        problem_type=\"single_label_classification\",\n","    )\n","\n","inference_model = AutoModelForSequenceClassification.from_pretrained(\n","    adapter_config.base_model_name_or_path,\n","    config=config,\n","    )\n","\n","label_to_id = {v: i for i, v in enumerate(label_list)}\n","\n","inference_model.config.label2id = label_to_id\n","inference_model.config.id2label = {id: label for label, id in label_to_id.items()}\n","tokenizer = AutoTokenizer.from_pretrained(adapter_config.base_model_name_or_path)\n","tokenizer.padding_side = \"left\"  \n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load the lora model\n","inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)\n","inference_model.resize_token_embeddings(len(tokenizer))\n","inference_model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# merged_model = inference_model.merge_and_unload() \n","\n","# trainer = Trainer(\n","#             model=inference_model,\n","#             args=training_args,\n","#             train_dataset=train_dataset if training_args.do_train else None,\n","#             eval_dataset=eval_dataset if training_args.do_eval else None,\n","#             compute_metrics=compute_metrics,\n","#             tokenizer=tokenizer,\n","#             data_collator=data_collator,\n","#             # callbacks=[WandbLoggingCallback()],\n","#         )\n","\n","# predictions = trainer.predict(predict_dataset)\n","# predictions.metrics['test_samples'] = len(predict_dataset)\n","# trainer.log_metrics(\"test\", predictions.metrics)\n","# trainer.save_metrics(\"test\", predictions.metrics)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["PeftModelForSequenceClassification(\n","  (base_model): LoraModel(\n","    (model): MistralForSequenceClassification(\n","      (model): MistralModel(\n","        (embed_tokens): Embedding(32000, 4096)\n","        (layers): ModuleList(\n","          (0-31): 32 x MistralDecoderLayer(\n","            (self_attn): MistralSdpaAttention(\n","              (q_proj): lora.Linear(\n","                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=128, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=128, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k_proj): lora.Linear(\n","                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=128, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=128, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (v_proj): lora.Linear(\n","                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=128, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=128, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (o_proj): lora.Linear(\n","                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=128, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=128, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (rotary_emb): MistralRotaryEmbedding()\n","            )\n","            (mlp): MistralMLP(\n","              (gate_proj): lora.Linear(\n","                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=128, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=128, out_features=14336, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (up_proj): lora.Linear(\n","                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=128, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=128, out_features=14336, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (down_proj): lora.Linear(\n","                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=14336, out_features=128, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=128, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (act_fn): SiLU()\n","            )\n","            (input_layernorm): MistralRMSNorm()\n","            (post_attention_layernorm): MistralRMSNorm()\n","          )\n","        )\n","        (norm): MistralRMSNorm()\n","      )\n","      (score): ModulesToSaveWrapper(\n","        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n","        (modules_to_save): ModuleDict(\n","          (default): Linear(in_features=4096, out_features=2, bias=False)\n","        )\n","      )\n","    )\n","  )\n",")"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["inference_model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bba39e7f2f3e4f119582ed2b64b19ec4","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["MistralForSequenceClassification(\n","  (model): MistralModel(\n","    (embed_tokens): Embedding(32000, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x MistralDecoderLayer(\n","        (self_attn): MistralSdpaAttention(\n","          (q_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (k_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (v_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (rotary_emb): MistralRotaryEmbedding()\n","        )\n","        (mlp): MistralMLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=14336, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=14336, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=14336, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): MistralRMSNorm()\n","        (post_attention_layernorm): MistralRMSNorm()\n","      )\n","    )\n","    (norm): MistralRMSNorm()\n","  )\n","  (score): Linear(in_features=4096, out_features=2, bias=False)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["adapter =  AutoModelForSequenceClassification.from_pretrained(\n","         pretrained_model_name_or_path=peft_model_id,\n","    )\n","adapter"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2e2598841f94dc4b6083842248f1efa","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["MistralForSequenceClassification(\n","  (model): MistralModel(\n","    (embed_tokens): Embedding(32000, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x MistralDecoderLayer(\n","        (self_attn): MistralSdpaAttention(\n","          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (rotary_emb): MistralRotaryEmbedding()\n","        )\n","        (mlp): MistralMLP(\n","          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): MistralRMSNorm()\n","        (post_attention_layernorm): MistralRMSNorm()\n","      )\n","    )\n","    (norm): MistralRMSNorm()\n","  )\n","  (score): Linear(in_features=4096, out_features=2, bias=False)\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model =  AutoModelForSequenceClassification.from_pretrained(\n","    adapter_config.base_model_name_or_path,\n","    config=config,\n","    )\n","model"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c104468274a24cb4b04fcea4d12b7b09","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["LlamaForSequenceClassification(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Identity()\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (k_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Identity()\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (v_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Identity()\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Identity()\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Identity()\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=11008, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Identity()\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=11008, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Identity()\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=11008, out_features=128, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=128, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (score): Linear(in_features=4096, out_features=9, bias=False)\n",")"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["peft_model_id = \"../../experiments_checkpoints/LoRA/meta_llama/patent_classification_abstract\"\n","adapter =  AutoModelForSequenceClassification.from_pretrained(\n","         pretrained_model_name_or_path=peft_model_id,\n","         num_labels=9,\n","    )\n","adapter"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'adapter' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madapter\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'adapter' is not defined"]}],"source":["adapter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del peft_model_id\n","del inference_model\n","del merged_model\n","del trainer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["236"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["import torch, gc\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/harpreet_guest2/akshat/Sequence_Classification/test\n"]},{"name":"stderr","output_type":"stream","text":["/home/harpreet_guest2/akshat/Sequence_Classification/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n","  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"]}],"source":["cd ../../test/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["'/home/harpreet_guest2/akshat/Sequence_Classification/notebook/lora'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOmUk1sIypJYfOJFc+kBw/D","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
